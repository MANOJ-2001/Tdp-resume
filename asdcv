use from databricks_langchain.chat_models, from databricks_langchain.embeddings import DatabricksEmbeddings import ChatDatabricks for authentication to databricks model and i will be using databricks-llama-4-maverick model for llm and databricks-bge-large-en for embedding model. will huggingface embeddings be used for embedding model or databricks embeddings? use databricks embeddings for embedding model and llm. be better is it free and secrure?
take these below files as refernce these are working i made a few chgangews in them but not wokring as we need. there are a lot of features mnissing in it and you know what are missing too. list them down
file names
sql_agent.py
sql_agent.py
intent_router.py
llm.py
import logging
from rag import get_relevant_context

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def format_sql_response(response: str) -> str:
    import re
    logger.info("Formatting SQL response.")
    response = re.sub(r"```sql[\s\S]*?```", lambda m: m.group(0)[6:-3], response, flags=re.IGNORECASE)
    response = re.sub(r"```[\s\S]*?```", lambda m: m.group(0)[3:-3], response, flags=re.IGNORECASE)
    match = re.search(r"(CREATE\s+OR\s+REPLACE\s+VIEW[\s\S]+)", response, re.IGNORECASE)
    if match:
        sql_script = match.group(1).strip()
        if ";" in sql_script:
            sql_script = sql_script[:sql_script.rfind(";")+1]
        logger.info("SQL script extracted.")
        return sql_script
    else:
        logger.info("No CREATE OR REPLACE VIEW found, returning raw response.")
        return response.strip()

def handle_sql_generation(user_input, chat_history, llm, memory=None):
    logger.info(f"Handling SQL generation for input: '{user_input}'")
    context_results = get_relevant_context(user_input)
    context = "\n".join([doc["content"] for doc in context_results])
    # Use memory for chat history
    if memory is not None:
        history_text = memory.load_memory_variables({})['history']
    else:
        history_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in chat_history])
    prompt = f"""You are a SQL generation assistant. Based on the user's request and the following chat history and policy context, generate an updated SQL view script.

Chat history:
{history_text}

User request:
{user_input}

Policy context:
{context}

Respond ONLY with the SQL script, starting with CREATE OR REPLACE VIEW ... AS SELECT. No explanations.
"""
    logger.info("Invoking LLM for SQL generation.")
    response = llm.invoke(
        prompt,
        max_tokens=512,
        temperature=0.7,
        top_p=0.9,
        stop=["\n", "User:", "AI:"]
    )
    logger.info("Received response from LLM.")
    return {
        "agent": "SQL Generation",
        "response": format_sql_response(response.content.strip()),
        "docs": context_results
    }

import logging
from config import EMBEDDING_ENDPOINT, EMBEDDING_TOKEN, INDEX_FOLDER
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from databricks_langchain.embeddings import DatabricksEmbeddings

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def index_policy_documents(docs_folder):
    logger.info(f"Indexing policy documents from folder: {docs_folder}")
    loader = DirectoryLoader(
        path=docs_folder,
        glob="**/*.txt",
        loader_cls=TextLoader
    )
    documents = loader.load()
    logger.info(f"Loaded {len(documents)} documents.")
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = splitter.split_documents(documents)
    logger.info(f"Split documents into {len(chunks)} chunks.")
    if not chunks:
        logger.error("No document chunks found.")
        raise ValueError("No document chunks found.")
    embedding = DatabricksEmbeddings(
        endpoint=EMBEDDING_ENDPOINT,
        api_token=EMBEDDING_TOKEN
    )
    logger.info("Loaded embedding model for indexing.")
    vectorstore = FAISS.from_documents(chunks, embedding)
    vectorstore.save_local(INDEX_FOLDER)
    logger.info(f"Indexed and saved to {INDEX_FOLDER}")

if __name__ == "__main__":
    index_policy_documents("path/to/knowledgebase")

import logging
from llm import get_llm

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def classify_intent(user_input, chat_history, llm, memory=None):
    logger.info(f"Classifying intent for input: '{user_input}'")
    # Use memory for chat history
    if memory is not None:
        history = memory.load_memory_variables({})['history']
    else:
        history = "\n".join([f"{msg['role']}: {msg['content']}" for msg in chat_history])
    prompt = f"""
You are an intent classifier. Classify the user's input into one of: knowledge_query, sql_generation, email_drafting, json_generation.
Respond with only the category name. No explanation.

Chat history:
{history}

User input: {user_input}
Intent:
"""
    response = llm.invoke(
        prompt,
        max_tokens=20,
        temperature=0.0,
        top_p=0.9,
        stop=["\n", "User:", "AI:"]
    )
    intent = response.content.strip()
    logger.info(f"Intent classified as: '{intent}'")
    return intent

def route_intent(user_input, chat_history, llm, memory=None):
    intent = classify_intent(user_input, chat_history, llm, memory=memory)
    logger.info(f"Routing to agent for intent: '{intent}'")
    if intent == "knowledge_query":
        from handle_normal_query import handle_normal_query
        return handle_normal_query(user_input, chat_history, llm, memory=memory)
    elif intent == "sql_generation":
        from handle_sql_generation import handle_sql_generation
        return handle_sql_generation(user_input, chat_history, llm, memory=memory)
    elif intent == "email_drafting":
        from handle_email_drafting import handle_email_drafting
        return handle_email_drafting(user_input, chat_history, llm, memory=memory)
    elif intent == "json_generation":
        from handle_json_generation import handle_json_generation
        return handle_json_generation(user_input, chat_history, llm, memory=memory)
    else:
        logger.warning("Intent not recognized. Asking user to rephrase.")
        return {
            "agent": "Unknown",
            "response": "Intent not recognized. Please rephrase your query.",
            "docs": []
        }

import logging
from config import LLM_ENDPOINT, LLM_TOKEN
from databricks_langchain.chat_models import ChatDatabricks

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def get_llm():
    logger.info("Initializing ChatDatabricks LLM with endpoint and token.")
    llm = ChatDatabricks(
        endpoint=LLM_ENDPOINT,
        api_token=LLM_TOKEN
    )
    logger.info("LLM initialized.")
    return llm
