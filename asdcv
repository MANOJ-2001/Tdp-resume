# 📁 memory.py
from langchain.memory import ConversationBufferMemory
from langchain.schema import messages_from_dict, messages_to_dict

# You can manage memory per user_id or session_id if needed
def get_memory(session_id=None):
    return ConversationBufferMemory(
        return_messages=True,
        memory_key="chat_history",
        input_key="query"
    )

# For more advanced cases, you could serialize/deserialize this using:
# messages_to_dict(memory.chat_memory.messages)
# messages_from_dict(serialized_data)


# 📁 graph.py (updated to use memory)
from langgraph.graph import StateGraph
from router import route_intent
from agents.general_agent import handle_general
from agents.sql_agent import handle_sql
from agents.email_agent import handle_email
from memory import get_memory

# Create workflow
workflow = StateGraph()

# Create memory
session_memory = get_memory()

# Define nodes
workflow.add_node("nlu", route_intent)
workflow.add_node("sql_agent", handle_sql)
workflow.add_node("email_agent", handle_email)
workflow.add_node("general_agent", handle_general)

# Entry point
workflow.set_entry_point("nlu")

# Route from NLU based on intent
workflow.set_conditional_edges(
    "nlu",
    lambda state: state.get("intent"),
    {
        "sql_agent": "sql_agent",
        "email_agent": "email_agent",
        "general_agent": "general_agent"
    }
)

# Set finish points
workflow.set_finish_point("sql_agent")
workflow.set_finish_point("email_agent")
workflow.set_finish_point("general_agent")

# Compile graph
app = workflow.compile()


# 📁 main.py (updated with memory)
from graph import app
from memory import get_memory

if __name__ == "__main__":
    memory = get_memory()

    while True:
        query = input("You: ")
        memory.save_context({"query": query}, {})
        response = app.invoke({"query": query, "chat_history": memory.chat_memory.messages})
        print("Agent:", response)
        memory.save_context({"query": query}, {"response": response})


# 📁 nlu.py
from llm import llm

def detect_intent(query: str) -> str:
    prompt = f"""
    Classify the user intent from the following:
    - sql
    - email
    - general
    Query: {query}
    Intent:
    """
    return llm.invoke(prompt).strip().lower()

def route_intent(state: dict) -> dict:
    query = state["query"]
    intent = detect_intent(query)
    return {**state, "intent": intent}


# 📁 llm.py
from langchain.llms import Databricks
from langchain.embeddings import HuggingFaceEmbeddings

llm = Databricks(
    endpoint="https://your-databricks-endpoint",
    api_token="your-token",
    model="llama-maverick-4"
)

embedding_model = HuggingFaceEmbeddings(model_name="bge-large-en")


# 📁 doc_loader.py
from langchain.document_loaders import TextLoader
from langchain.vectorstores import FAISS
from llm import embedding_model
import os

def embed_and_store(doc_path, index_path="./indexes"):
    loader = TextLoader(doc_path)
    documents = loader.load()
    vectordb = FAISS.from_documents(documents, embedding_model)
    vectordb.save_local(index_path)
    print("Documents embedded and saved.")


# 📁 rag.py
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from llm import llm, embedding_model

def load_index(index_path="./indexes"):
    return FAISS.load_local(index_path, embedding_model)

def get_rag_response(query):
    db = load_index()
    retriever = db.as_retriever()
    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)
    return qa_chain.run(query)


# 📁 agents/general_agent.py
from rag import get_rag_response

def handle_general(state: dict) -> str:
    query = state["query"]
    response = get_rag_response(query)
    return response


# 📁 agents/sql_agent.py
from llm import llm

def handle_sql(state: dict) -> str:
    query = state["query"]
    prompt = f"You are a SQL view generator. Based on this request, create SQL: {query}"
    return llm.invoke(prompt)


# 📁 agents/email_agent.py
from llm import llm

def handle_email(state: dict) -> str:
    query = state["query"]
    prompt = f"You are an email assistant. Draft a professional email for: {query}"
    return llm.invoke(prompt)
